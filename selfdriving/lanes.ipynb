{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#import cv2 for breaking images down to pixels or continuous arrays\n",
    "img = cv2.imread('test_image.jpg')\n",
    "#It's imparative to make a copy of image, in\n",
    "lane_img = np.copy(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def canny(image):\n",
    "    #To convert the color of image from a RGB to grayscale\n",
    "    #cuz the number of channel of a RGB image is 3, whereas grayscale channel is 1,\n",
    "    #Thus grayscale image enables to run itself faster than RGB\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    #for reducing the noises of the image, we use GaussianBlur\n",
    "    #in which the algorhthm takes averages of the adjacent pixels and \n",
    "    #average them\n",
    "    blur = cv2.GaussianBlur(gray,(5,5),0)\n",
    "    #isolate the edge in images, we are using the derivative of the adjacent pixels\n",
    "    #thus we know the slope of specific points \n",
    "    #images are arrays of consective pixels, \n",
    "    # we can denote like this pixel = (x,y)\n",
    "    #if a griedient of the adjacent pixels is lower than lowest_thrushhold\n",
    "    #It'll be ingored \n",
    "    #if a griedient of the adjacent pixels is higher than highest_thrushhold\n",
    "    #It's gonna be itentified as an edge\n",
    "    #If a gredient of adjacent pixels is between lowest and highest_thrushhold\n",
    "    #It'll be identified as edges, only if it's placed next to the edges\n",
    "    #The documentation recommends to use the ratio of thrushholds 1:3\n",
    "    canny = cv2.Canny(blur,50,150)\n",
    "    return canny\n",
    "\n",
    "\n",
    "def region_of_interest(image):\n",
    "    #to limit our images for an extent where we can identify lanes\n",
    "    #first, we specify the area of images \n",
    "\n",
    "    #get the rows from image's shape\n",
    "    height = image.shape[0]\n",
    "    #specify the cordinates where you want to idetify objects\n",
    "    polygons = np.array([\n",
    "    [(200,height),(1100,height),(550,250)]\n",
    "    ])\n",
    "    #then make a copy of image in black, then mask with the polygon we made above\n",
    "    mask = np.zeros_like(image)\n",
    "    cv2.fillPoly(mask,polygons,255)\n",
    "    \n",
    "    #8 bit binary representation's maximum number that can show\n",
    "    # is 255. When a pixel is completely black, the binary representation\n",
    "    # should be 00000000. When we operate bitwise-and to the corresponding\n",
    "    # region of the other image, All of the pixels which are at same matrix\n",
    "    # will be all zero(0). Cause in bitwise-and 1 applied only if the bit value\n",
    "    # of two images are both 1, otherwise 0. \n",
    "    masked_image= cv2.bitwise_and(mask,image)\n",
    "    \n",
    "    # lines detection\n",
    "    # we detect lines in region of interest.\n",
    "    # lines are a series of dots. then to find the line's equasion by using \n",
    "    # hough line, which detects intersections --with params (radians,distance)\n",
    "    # the line pass through the dots/points \n",
    "    # divided houghline to bins, then take the most intersections including bin\n",
    "    # the params in that bin tends to pass  \n",
    "    lines = cv2.HoughLinesP(masked_image,2,np.pi/180,100,np.array([]),minLineLength = 40, maxLineGap =5)\n",
    "    return masked_image,lines\n",
    "\n",
    "def lines_onto_black_image(image,lines):\n",
    "    #create completely black image with the same dimention as image\n",
    "    lines_onto_black_image = np.zeros_like(image)\n",
    "    #just confirm that we detected the lines with houghlinep\n",
    "    if lines is not None:\n",
    "        for l in lines:\n",
    "            # raw coordinates are 3 dimentional array like below\n",
    "                #[[[,,,,]]\n",
    "                # [[,,,,]]\n",
    "                # ]\n",
    "            # in for loop these convert to 2 dimentions\n",
    "            # then we wanna use 1 dimention, 4 coordinates then reshape\n",
    "            x1,y1,x2,y2 = l.reshape(4)\n",
    "            # after that cv2.line allows us to draw lines between 2 points (x1,y1),(x2,y2)\n",
    "            cv2.line(lines_onto_black_image,(x1,y1),(x2,y2),(255,0,0),10)\n",
    "        return lines_onto_black_image\n",
    "    \n",
    "def average_slope_intercept(image,lines):\n",
    "    # for smoothing the lines\n",
    "    # average slope and intercept\n",
    "    left_line = []\n",
    "    right_line = []\n",
    "    \n",
    "    for l in lines:\n",
    "        x1,y1,x2,y2 = l.reshape(4)\n",
    "        #polyfit returns first degree polynomial's coefficients\n",
    "        #but depending on argument of deg(degree)\n",
    "        #first degree means the equation with 1 exponential\n",
    "        paramators = np.polyfit((x1,x2),(y1,y2),1)\n",
    "        slope = paramators[0]\n",
    "        intercepts = paramators[1]\n",
    "    \n",
    "    #as for this image, two lines slanted to the right, and left respectively\n",
    "    #y axis of image from the top of left side downward to the bottum of left\n",
    "    #so the slope of the line slanted to the right is negative\n",
    "    #the slope of the line slanted to the left is positive\n",
    "        if slope < 0:\n",
    "            left_line.append((slope, intercepts))\n",
    "        else:\n",
    "            right_line.append((slope, intercepts))\n",
    "    #then average out each slope and intercepts respectively\n",
    "    left_line_average = np.average(left_line,axis=0)\n",
    "    right_line_average = np.average(right_line,axis=0)\n",
    "    ave_left_line = get_coordinates(image,left_line_average)\n",
    "    ave_right_line = get_coordinates(image,right_line_average)\n",
    "    #[308 704 483 422] [978 704 703 422]\n",
    "    #as seen above, returned values are the coordinates of each line--left/right\n",
    "    return ave_left_line, ave_right_line\n",
    "    \n",
    "def get_coordinates(image,line_params):\n",
    "    slope, intercepts = line_params\n",
    "    y1 = image.shape[0]\n",
    "    y2 = int(y1*3/5)\n",
    "    x1 = int((y1-intercepts)/slope)\n",
    "    x2 = int((y2-intercepts)/slope)\n",
    "    return np.array([x1,y1,x2,y2])\n",
    "\n",
    "# canny = canny(lane_img)\n",
    "\n",
    "# masked_image,lines = region_of_interest(canny)\n",
    "# averaged_lines = average_slope_intercept(lane_image,lines)\n",
    "# lines_onto_black_image = lines_onto_black_image(lane_img,averaged_lines)\n",
    "# #blend the first lane_image and the lines_image\n",
    "# #in lines_image whole region is black other than lines \n",
    "# #it means pixels intensities equal to 0\n",
    "# #therefore adding pixels intensities of black space won't make\n",
    "# #any differences, only when add the intensities of lines we can cee\n",
    "# #them on raw images\n",
    "# blended_image = cv2.addWeighted(lane_img,0.8, lines_onto_black_image,1,1)\n",
    "\n",
    "# cv2.imshow('lanes',blended_image )\n",
    "# cv2.waitKey(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ryona\\learning\\selfdriving\\driving\\lib\\site-packages\\numpy\\lib\\function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "c:\\Users\\ryona\\learning\\selfdriving\\driving\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable numpy.float64 object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ryona\\learning\\selfdriving\\lanes.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ryona/learning/selfdriving/lanes.ipynb#ch0000003?line=35'>36</a>\u001b[0m \u001b[39m# lines detection\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ryona/learning/selfdriving/lanes.ipynb#ch0000003?line=36'>37</a>\u001b[0m \u001b[39m# we detect lines in region of interest.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ryona/learning/selfdriving/lanes.ipynb#ch0000003?line=37'>38</a>\u001b[0m \u001b[39m# lines are a series of dots. then to find the line's equasion by using \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ryona/learning/selfdriving/lanes.ipynb#ch0000003?line=40'>41</a>\u001b[0m \u001b[39m# divided houghline to bins, then take the most intersections including bin\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ryona/learning/selfdriving/lanes.ipynb#ch0000003?line=41'>42</a>\u001b[0m \u001b[39m# the params in that bin tends to pass lines more than other bins do\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ryona/learning/selfdriving/lanes.ipynb#ch0000003?line=42'>43</a>\u001b[0m lines \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mHoughLinesP(masked_image,\u001b[39m2\u001b[39m,np\u001b[39m.\u001b[39mpi\u001b[39m/\u001b[39m\u001b[39m180\u001b[39m,\u001b[39m100\u001b[39m,np\u001b[39m.\u001b[39marray([]),minLineLength \u001b[39m=\u001b[39m \u001b[39m40\u001b[39m, maxLineGap \u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ryona/learning/selfdriving/lanes.ipynb#ch0000003?line=43'>44</a>\u001b[0m averaged_lines \u001b[39m=\u001b[39m average_slope_intercept(frame,lines)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ryona/learning/selfdriving/lanes.ipynb#ch0000003?line=44'>45</a>\u001b[0m \u001b[39m# lines_onto_black_image = lines_onto_black_image(frame,averaged_lines)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ryona/learning/selfdriving/lanes.ipynb#ch0000003?line=45'>46</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ryona/learning/selfdriving/lanes.ipynb#ch0000003?line=46'>47</a>\u001b[0m     \u001b[39m#create completely black image with the same dimention as image\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ryona/learning/selfdriving/lanes.ipynb#ch0000003?line=47'>48</a>\u001b[0m black_image \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros_like(frame)\n",
      "\u001b[1;32mc:\\Users\\ryona\\learning\\selfdriving\\lanes.ipynb Cell 5\u001b[0m in \u001b[0;36maverage_slope_intercept\u001b[1;34m(image, lines)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ryona/learning/selfdriving/lanes.ipynb#ch0000003?line=97'>98</a>\u001b[0m left_line_average \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39maverage(left_line,axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ryona/learning/selfdriving/lanes.ipynb#ch0000003?line=98'>99</a>\u001b[0m right_line_average \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39maverage(right_line,axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/ryona/learning/selfdriving/lanes.ipynb#ch0000003?line=99'>100</a>\u001b[0m ave_left_line \u001b[39m=\u001b[39m get_coordinates(image,left_line_average)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ryona/learning/selfdriving/lanes.ipynb#ch0000003?line=100'>101</a>\u001b[0m ave_right_line \u001b[39m=\u001b[39m get_coordinates(image,right_line_average)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ryona/learning/selfdriving/lanes.ipynb#ch0000003?line=101'>102</a>\u001b[0m \u001b[39m#[308 704 483 422] [978 704 703 422]\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ryona/learning/selfdriving/lanes.ipynb#ch0000003?line=102'>103</a>\u001b[0m \u001b[39m#as seen above, returned values are the coordinates of each line--left/right\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\ryona\\learning\\selfdriving\\lanes.ipynb Cell 5\u001b[0m in \u001b[0;36mget_coordinates\u001b[1;34m(image, line_params)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ryona/learning/selfdriving/lanes.ipynb#ch0000003?line=105'>106</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_coordinates\u001b[39m(image,line_params):\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/ryona/learning/selfdriving/lanes.ipynb#ch0000003?line=106'>107</a>\u001b[0m     slope, intercepts \u001b[39m=\u001b[39m line_params\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ryona/learning/selfdriving/lanes.ipynb#ch0000003?line=107'>108</a>\u001b[0m     y1 \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ryona/learning/selfdriving/lanes.ipynb#ch0000003?line=108'>109</a>\u001b[0m     y2 \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(y1\u001b[39m*\u001b[39m\u001b[39m3\u001b[39m\u001b[39m/\u001b[39m\u001b[39m5\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable numpy.float64 object"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#video line detection \n",
    "#capture the video, and while it's run as each single frame\n",
    "#detect and get 2 values(boolean, frame)\n",
    "cap = cv2.VideoCapture('test2.mp4')\n",
    "while(cap.isOpened()):\n",
    "    ret,frame = cap.read()\n",
    "    if ret == True:\n",
    "        # then detect lines and project it on video frame/image\n",
    "        # Just changed the input image from lane_image to frame\n",
    "        # canny = canny(frame)\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        blur = cv2.GaussianBlur(gray,(5,5),0)\n",
    "        canny = cv2.Canny(blur,50,150)\n",
    "\n",
    "                #to limit our images for an extent where we can identify lanes\n",
    "        #first, we specify the area of images \n",
    "\n",
    "        #get the rows from image's shape\n",
    "        height = canny.shape[0]\n",
    "        #specify the cordinates where you want to idetify objects\n",
    "        polygons = np.array([\n",
    "        [(200,height),(1100,height),(550,250)]\n",
    "        ])\n",
    "        #then make a copy of image in black, then mask with the polygon we made above\n",
    "        mask = np.zeros_like(canny)\n",
    "        cv2.fillPoly(mask,polygons,255)\n",
    "        \n",
    "        #8 bit binary representation's maximum number that can show\n",
    "        # is 255. When a pixel is completely black, the binary representation\n",
    "        # should be 00000000. When we operate bitwise-and to the corresponding\n",
    "        # region of the other image, All of the pixels which are at same matrix\n",
    "        # will be all zero(0). Cause in bitwise-and 1 applied only if the bit value\n",
    "        # of two images are both 1, otherwise 0. \n",
    "        masked_image= cv2.bitwise_and(mask,canny)\n",
    "        \n",
    "        # lines detection\n",
    "        # we detect lines in region of interest.\n",
    "        # lines are a series of dots. then to find the line's equasion by using \n",
    "        # hough line, which detects intersections --with params (radians,distance)\n",
    "        # the line pass through the dots/points \n",
    "        # divided houghline to bins, then take the most intersections including bin\n",
    "        # the params in that bin tends to pass lines more than other bins do\n",
    "        lines = cv2.HoughLinesP(masked_image,2,np.pi/180,100,np.array([]),minLineLength = 40, maxLineGap =5)\n",
    "        averaged_lines = average_slope_intercept(frame,lines)\n",
    "        # lines_onto_black_image = lines_onto_black_image(frame,averaged_lines)\n",
    "        \n",
    "            #create completely black image with the same dimention as image\n",
    "        black_image = np.zeros_like(frame)\n",
    "    #just confirm that we detected the lines with houghlinep\n",
    "        if lines is not None:\n",
    "            for l in lines:\n",
    "                # raw coordinates are 3 dimentional array like below\n",
    "                    #[[[,,,,]]\n",
    "                    # [[,,,,]]\n",
    "                    # ]\n",
    "                # in for loop these convert to 2 dimentions\n",
    "                # then we wanna use 1 dimention, 4 coordinates then reshape\n",
    "                x1,y1,x2,y2 = l.reshape(4)\n",
    "                # after that cv2.line allows us to draw lines between 2 points (x1,y1),(x2,y2)\n",
    "                cv2.line(black_image,(x1,y1),(x2,y2),(255,0,0),10)\n",
    "        #blend the first lane_image and the lines_image\n",
    "        #in lines_image whole region is black other than lines \n",
    "        #it means pixels intensities equal to 0\n",
    "        #therefore adding pixels intensities of black space won't make\n",
    "        #any differences, only when add the intensities of lines we can cee\n",
    "        #them on raw images\n",
    "        blended_image = cv2.addWeighted(frame,0.8, black_image,1,1)\n",
    "\n",
    "        cv2.imshow('lanes',blended_image )\n",
    "    # Press Q on keyboard to  exit\n",
    "    \n",
    "        if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "  # Break the loop\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# When everything done, release the video capture object\n",
    "\n",
    "cap.release()\n",
    "# Closes all the frames\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "driving",
   "language": "python",
   "name": "driving"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "71896b83fb11a20e0bdba9d1ee773c5cd354f455a6d9eb99f2ec9fd0303bdaa1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
